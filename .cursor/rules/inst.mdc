# Hands-On Skill Pathway Repository

## Production Implementation Guide for Developers

This document defines the **complete architecture, structure, automation, and evaluation system** for a hands-on, skills-based learning and verification repository.

This repository is not just a course workspace. It is a **self-assessing skill verification system** where learning, practice, automated evaluation, and public evidence of competency all exist together.

---

# 1. SYSTEM PURPOSE

Each repository represents **one skill pathway** (example: *Modern React Engineer*).

Within the pathway:

- Each **course** represents a major skill area
- Each course contains a **standalone working project**
- Each project contains **multiple challenges**
- Each challenge is automatically reviewed
- Results generate **verifiable skill evidence**

This system must be:

- Fully automated
- Deterministic + AI-assisted
- Tamper-resistant
- Publicly verifiable
- Production scalable

---

# 2. PATHWAY INCLUDED IN THIS VERSION

This example pathway contains **3 courses**:

1. React Fundamentals
2. RTK Query (Redux Toolkit Query)
3. Next.js App Router

Each course includes **3 sample challenges** for structure demonstration.

---

# 3. REPOSITORY ROOT STRUCTURE

```
skill-pathway-modern-react/
│
├── courses/
│   ├── 01-react-fundamentals/
│   ├── 02-rtk-query/
│   └── 03-nextjs-app-router/
│
├── pathway-review/
│   ├── pathway-summary.json
│   ├── skill-breakdown.json
│   └── pathway-config.json
│
├── global-review/
│   ├── scoring-engine/
│   ├── ai-review/
│   └── run-all-reviews.js
│
├── learner-results/ (auto-generated global cache)
│
├── .github/workflows/
│   └── solo-skill-review.yml
│
└── README.md
```

---

# 4. COURSE FOLDER STRUCTURE (APPLIES TO ALL COURSES)

Example: `courses/01-react-fundamentals/`

```
01-react-fundamentals/
│
├── project/                  → Standalone runnable app
├── challenges/               → Individual skill tasks
├── review-engine/            → Deterministic evaluation logic
├── ai-review/                → AI evaluation prompts + adapters
├── results/                  → Auto-generated evaluation output
│   ├── challenge-results.json
│   ├── course-summary.json
│   └── ai-feedback.json
└── course-config.json        → Scoring + skill definitions
```

Each course must be independently testable and independently badgeable.

---

# 5. PROJECT REQUIREMENTS PER COURSE

Each `project/` folder must:

- Be a fully runnable application
- Have its own package.json
- Include linting configuration
- Include test framework setup
- Use realistic folder structure

### Course Project Types

| Course             | Project Type                  |
| ------------------ | ----------------------------- |
| React Fundamentals | Component-based UI app        |
| RTK Query          | Data-driven dashboard         |
| Next.js            | Fullstack app with App Router |

---

# 6. CHALLENGE STRUCTURE

Each course has **3 challenges (sample)**:

```
challenges/
├── 01-
├── 02-
└── 03-
```

Inside each challenge:

```
challenge-folder/
├── README.md            → Problem statement + Technical Requirements (acceptance criteria)
├── starter/             → Starter code
├── tests/               → Deterministic evaluation tests
└── metadata.json        → Scoring rules
```

Learner modifies only files inside `starter/` or as instructed.

---

# 7. EVALUATION LAYERS (PER CHALLENGE)

Each challenge is evaluated using **multi-signal scoring**:

1. Functional correctness (unit/integration tests)
2. Code quality (linting + static analysis)
3. Architecture & pattern validation
4. Best practices & performance heuristics
5. AI qualitative engineering review

All signals combine into **one final score**.

---

# 8. DETERMINISTIC REVIEW ENGINE

Each course contains a `review-engine/` module.

Responsibilities:

- Run tests
- Run linting
- Validate file structure
- Perform AST pattern checks
- Compute numeric scores

Output per challenge written to:

`results/challenge-results.json`

---

# 9. AI REVIEW LAYER

Each course contains an `ai-review/` module.

Responsibilities:

- Analyze learner code quality
- Evaluate readability and maintainability
- Provide structured improvement suggestions

AI results written to:

`results/ai-feedback.json`

AI NEVER overrides failing functional tests.

---

# 10. COURSE SUMMARY GENERATION

After challenge evaluations:

`results/course-summary.json` must be generated containing:

- Completion %
- Average score
- Skill strengths
- Improvement areas
- Badge level recommendation

This file is the **primary evidence for course-level badge**.

---

# 11. PATHWAY AGGREGATION

Global script in `/global-review/run-all-reviews.js` must:

1. Trigger each course review engine
2. Trigger each course AI review
3. Update all course result files
4. Aggregate pathway-level metrics

Outputs stored in:

`/pathway-review/pathway-summary.json`

This file is the **primary evidence for pathway badge**.

---

# 12. AUTOMATION VIA GITHUB ACTIONS

File: `.github/workflows/solo-skill-review.yml`

Triggered on every push.

Steps:

1. Checkout code
2. Install dependencies
3. Run global review engine
4. Commit updated results files

Result files must always be system-generated.

---

# 13. TAMPER RESISTANCE

The system must ensure:

- Learner edits to results files are overwritten
- Tests are restored if deleted
- Only automated workflow updates results

---

# 14. BADGE EVIDENCE SOURCES

| Badge Type | Evidence File          |
| ---------- | ---------------------- |
| Challenge  | challenge-results.json |
| Course     | course-summary.json    |
| Pathway    | pathway-summary.json   |

All badges reference:

- Repo URL
- Commit SHA
- Signed record stored on issuing platform

---

# 15. FUTURE EXTENSIONS

This architecture must allow:

- Adding new courses easily
- Adding AI evaluation enhancements
- Adding plagiarism detection
- Adding performance benchmarking
- Adding employer-facing verification tools

---

# FINAL NOTE FOR DEVELOPERS

This repository is both:

1. A hands-on learning workspace
2. A machine-verifiable skill evidence generator

Build all systems with production quality, modularity, and automation in mind.

Every evaluation step must be reproducible and deterministic before AI augmentation.

---

# 16. CURSOR / AI CODE GENERATION MASTER PROMPT

Use the following prompt when generating this repository with an AI coding tool (such as Cursor). This prompt instructs the AI to build the FULL production-ready system — not just sample code.

---

**AI BUILD PROMPT START**

You are building a **production-grade, automated skill assessment repository** for a hands-on developer learning pathway. This is not a tutorial repo — it is a self-evaluating system that verifies real engineering competency.

The repository represents **ONE SKILL PATHWAY** called: **"Modern React Engineer"**

This pathway must contain **THREE COURSES**, each with its own standalone working project and automated review system:

1. React Fundamentals
2. RTK Query (Redux Toolkit Query)
3. Next.js App Router

Each course must include **THREE SAMPLE CHALLENGES**.

This system must be designed as **production-ready infrastructure**, not demo code.

---

## CORE SYSTEM REQUIREMENTS

The repository must:

- Be fully automated
- Run evaluations on every push via GitHub Actions
- Combine deterministic testing + static analysis + architecture checks + AI review
- Store machine-readable results as evidence
- Be modular and scalable for adding more courses later

---

## REPOSITORY STRUCTURE

Create the full folder structure exactly as defined below, with working code, configs, and scripts:

courses/ 01-react-fundamentals/ 02-rtk-query/ 03-nextjs-app-router/

Each course must contain:

- project/ (fully runnable app)
- challenges/ (3 challenges)
- review-engine/ (Node-based evaluation engine)
- ai-review/ (AI evaluation integration layer)
- results/ (auto-generated JSON outputs)
- course-config.json (scoring configuration)

Also create:

- global-review/ (pathway-level aggregation engine)
- pathway-review/ (pathway summary outputs)
- .github/workflows/solo-skill-review\.yml (automation)

---

## PROJECT IMPLEMENTATION RULES

Each course project must:

- Use modern tooling
- Have linting configured
- Have a testing framework configured
- Be structured like a real production project

Tech stack expectations:

- React courses → Vite + React + TypeScript
- RTK Query course → Redux Toolkit + RTK Query + API mocks
- Next.js course → Next.js App Router + API routes

---

## CHALLENGE SYSTEM

Each challenge must include:

- README.md (problem statement + Technical Requirements section for acceptance criteria)
- starter/ (editable learner code)
- tests/ (automated verification)
- metadata.json (scoring weights)

---

## DETERMINISTIC REVIEW ENGINE

Build a Node.js-based review engine per course that:

- Runs tests
- Runs ESLint
- Validates folder structure
- Uses AST parsing to detect required patterns
- Produces numeric scores

Write results to: results/challenge-results.json results/course-summary.json

---

## AI REVIEW LAYER

Create an AI evaluation module that:

- Reviews code readability
- Evaluates maintainability
- Detects over-engineering
- Produces structured feedback

AI results must NEVER override failed functional tests.

---

## PATHWAY AGGREGATION ENGINE

Create a global script that:

- Runs all course review engines
- Runs AI review for each course
- Aggregates pathway scores
- Outputs pathway-review/pathway-summary.json

---

## AUTOMATION

Create a GitHub Action that:

- Runs on every push
- Installs dependencies
- Runs the global review engine
- Commits updated results files automatically

---

## PRODUCTION-READINESS REQUIREMENTS

All code must be written as if this system will scale to thousands of learners:

- Modular design
- Clear separation of evaluation layers
- Config-driven scoring
- No hardcoded assumptions
- Proper error handling
- Clean logging
- Extensible architecture

Do NOT write placeholder stubs. Implement working pipelines, scripts, and configs.

This repository is both a **learning environment** and a **verifiable engineering assessment system**.
 for Ai integration use below credentials
 name: Groq Llama 3.1 8B (Fast)
    provider: groq
    model: llama-3.1-8b-instant
    apiKey: YOUR_GROQ_API_KEY
